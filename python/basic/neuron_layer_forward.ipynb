{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(inputs, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    weights = np.random.rand(len(inputs))\n",
    "    bias = np.random.rand(1)\n",
    "    \n",
    "    forward = np.dot(inputs, weights) + bias\n",
    "    \n",
    "    # forward2 = 0\n",
    "    # for i in range(len(inputs)):\n",
    "    #     forward2 += weights[i] * inputs[i]\n",
    "    \n",
    "    # forward2+=bias\n",
    "    \n",
    "    return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.02260313])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 4]\n",
    "neuron(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.3906699 , 3.2141003 , 4.29849217])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer(inputs, num_neurons):\n",
    "    outs = []\n",
    "    for i in range(num_neurons):\n",
    "        outs.append(neuron(inputs, random_state=i).item())\n",
    "    \n",
    "    return np.array(outs)\n",
    "\n",
    "layer(inputs, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(inputs, num_neurons, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    weights = np.random.random((num_neurons, len(inputs)))\n",
    "    bias = np.random.random(num_neurons)\n",
    "    \n",
    "    outs = np.dot(weights, inputs) + bias\n",
    "    \n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.69902713, 4.31930221, 6.14047803])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(inputs=inputs, num_neurons=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[1, 2, 3, 4, 5],\n",
    "          [3, 4, 5, 6, 7],\n",
    "          [5, 6, 7, 8, 9]]\n",
    "\n",
    "inputs = np.array(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.55008407,  5.51307186,  5.32913718,  9.76745019],\n",
       "       [ 9.02123875,  9.24491832,  9.60503551, 15.28272647],\n",
       "       [12.49239344, 12.97676478, 13.88093383, 20.79800275]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.random((4, len(inputs[0])))\n",
    "b = np.random.random(4)\n",
    "x = np.dot(inputs, w.T)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80839735, 0.30461377, 0.09767211, 0.68423303])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.35848141,  5.81768563,  5.42680929, 10.45168322],\n",
       "       [ 9.8296361 ,  9.54953209,  9.70270762, 15.9669595 ],\n",
       "       [13.30079079, 13.28137855, 13.97860595, 21.48223577]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_layer(inputs, num_neurons, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    weights = np.random.random((num_neurons, len(inputs[0])))\n",
    "    bias = np.random.random(num_neurons)\n",
    "    \n",
    "    outs = np.dot(inputs, weights.T) + bias\n",
    "    \n",
    "    return outs\n",
    "\n",
    "batch_layer(inputs, num_neurons=7).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_mlp(inputs, num_layers, neurons_in_layers, batched=False, random_state=42):\n",
    "    inputs = np.array(inputs)\n",
    "    assert num_layers == len(neurons_in_layers), \"The neurons in layers should be equal to the number of layers\"\n",
    "    print(inputs.shape)\n",
    "    # to handle batched inputs\n",
    "    if batched:\n",
    "        outs = batch_layer(inputs, neurons_in_layers[0], random_state=random_state)\n",
    "        print(outs.shape)\n",
    "        for i in range(1, num_layers):\n",
    "            outs = batch_layer(outs, neurons_in_layers[i], random_state=i)\n",
    "            print(outs.shape)\n",
    "            \n",
    "    # to handle single inputs\n",
    "    else:\n",
    "        outs = layer(inputs, neurons_in_layers[0], random_state=random_state)\n",
    "        print(outs.shape)\n",
    "        for i in range(1, num_layers):\n",
    "            outs = layer(outs, neurons_in_layers[i], random_state=i)\n",
    "            print(outs.shape)\n",
    "    \n",
    "    \n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5)\n",
      "(3, 4)\n",
      "(3, 6)\n",
      "(3, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[31.8447584 , 33.51619007],\n",
       "       [50.20419367, 52.51219789],\n",
       "       [68.56362894, 71.50820572]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_mlp(inputs, 3, [4, 6, 2], batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_inputs = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(6,)\n",
      "(7,)\n",
      "(8,)\n",
      "(2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([178.82156503, 136.82566075])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_mlp(linear_inputs, 4, [6, 7, 8, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
